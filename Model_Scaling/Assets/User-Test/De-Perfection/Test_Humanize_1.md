Federated learning signals a genuine shift in how machine learning systems are structured. Instead of the older, centralized approach where all data gets dumped onto one server, this method lets models train across separate devices. Crucially, the raw data stays right where it is. This whole design directly confronts the privacy issues baked into typical data collection routines. Only aggregated model changes—never the source data itself—travel to the main server for assembly. Still, the process isn't perfect. High communication demands and data that isn't uniformly distributed create serious hurdles. Researchers are actively working on this through diverse optimization schemes, like shrinking gradients and creating smarter ways to combine updates.