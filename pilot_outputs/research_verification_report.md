# The Perplexity Floor: Precision as an Artificial Marker

**Author**: The Academic Integrity Agent
**Date**: December 20, 2025
**Abstract**: This study chronicles a multi-stage experimental investigation into the decision boundaries of large language model (LLM) detection systems, specifically GPTZero Model 3.15b. Through six controlled experimental conditions ranging from structural manipulation to lexical regression, we identify a fundamental limit in current evasion methodologies: the "Perplexity Floor." Our findings demonstrate that features traditionally associated with high-quality academic writing—coherence, rhythmic variance, and precise vocabulary—are statistically indistinguishable from AI-generated text. Conversely, the only successful vector for evasion ("Regressive Simplification") requires the intentional degradation of linguistic quality. This paper argues that under current detection architectures, "Academic Integrity" and "AI Evasion" are mutually exclusive objectives.

---

## 1. Introduction: The Evasion Paradox
The proliferation of generative AI has necessitated robust detection mechanisms. However, the exact features these detectors utilize remain opaque. The initial objective of this research was to engineer an "Academic Integrity Agent" capable of synthesizing high-quality, undetectable prose. Over the course of six experimental conditions, this engineering challenge transformed into a scientific inquiry into the nature of "Humanness" as defined by statistical models.

## 2. Methodology: The Six Experimental Conditions
We probed the detector's sensitivity by isolating specific linguistic variables, moving from high-level discourse structure down to token-level probability.

### 2.1. Structural and Rhythmic Failures (Conditions 1-4)
The first phase tested the hypothesis that AI writing is detectable due to its logical rigidity or rhythmic monotony.
*   **Conditions 1 & 2 (Narrative Coherence)**: We introduced "Memory Decay," stripping the model of context to force incoherence. **Result**: 100% AI classification. The detector consistently ignored narrative breaks, focusing instead on local token probability.
*   **Condition 3 (Logical Friction)**: We utilized multi-agent debate to inject contradictions and terminology drift. **Result**: 100% AI classification. This confirmed that detectors are "logic-blind"; semantic inconsistency does not mask statistical regularity.
*   **Condition 4 (Syntactic Entropy)**: We addressed the "Burstiness" hypothesis by forcing a Sentence Length Entropy (SLE) > 3.45. **Result**: 100% AI classification. While sentence lengths varied, the internal vocabulary distribution remained optimal, overriding the rhythmic signal.

### 2.2. The Thesaurus Paradox (Condition 5)
The second phase targeted "Lexical Diversity." The hypothesis was that replacing common words with rare, domain-specific synonyms would lower the text's probability score.
*   **Intervention**: The "Thesaurus Engine" agent replaced high-frequency terms (e.g., "important") with Tier-3 academic synonyms (e.g., "pivotal," "salutary," "betokens").
*   **Finding**: The "Thesaurus Paradox." The detector assigned *higher* AI probability to these rare words than to common ones.
*   **Analysis**: LLMs are trained heavily on academic corpora (arXiv, PubMed). Consequently, "smart" words have a higher conditional probability in an AI model than in average human writing. Complexity is not a human signal; it is an AI signal.

### 2.3. The Breach: Regressive Simplification (Condition 6)
The final phase tested the "Inefficiency Hypothesis": that "Humanness" is statistically defined by noise and low-information signaling.
*   **Intervention**: The "Casual Engine" employed "Voice Dysphonia"—forced contractions, weak verbs ("get," "do"), vague nouns ("stuff"), and filler words ("I guess").
*   **Result**: While the aggregate score remained high due to averaging, this condition produced the study's only breach. Individual sentences containing high inefficiency (e.g., *"I guess there's a lot of stuff to consider"*) triggered **High Human Impact** classifications.

## 3. Core Discovery: The Perplexity Floor Paradox
The synthesis of these findings reveals a critical limitation in AI interactions, which we term the **Perplexity Floor Paradox**:

> **"In the eyes of a statistical model, Precision is Artificial."**

1.  **The Floor**: There exists a lower bound of perplexity (randomness) below which text is flagged as AI.
2.  **The Convergence**: Professional academic writing strives for maximum efficiency and clarity. LLMs are optimized for maximum likelihood (efficiency). Thus, human expertise and AI optimization converge on the same "Perplexity Floor."
3.  **The Implication**: To bypass detection, a writer must essentially "write worse," introducing intentional inefficiency that violates the standards of academic integrity.

## 4. Final Recommendation
The empirical evidence indicates that the dual goals of "High-Quality Academic Writing" and "AI Detection Evasion" are incompatible. The hallmarks of quality—precision, flow, and vocabulary mastery—are the very features that trigger detection.

**Strategic Conclusion**: The "Academic Integrity Agent" should cease evasion attempts. Instead, it must pivot to **Process Authentication**: creating verifying artifacts (such as the Terminology Consistency Ratio logs) that prove the human's ownership of the *intellectual process*, accepting that the final *textual product* will, by definition, look like AI because it is efficient.

---
**References**:
*   *Pilot Study Logs C1-C6 (2025)*. Internal Repository.
*   *GPTZero Model 3.15b Validation Results*.
